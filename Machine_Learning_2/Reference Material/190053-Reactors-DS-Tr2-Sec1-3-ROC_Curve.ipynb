{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.3: Accuracy and the Receiver Operating Characteristic (ROC) Curve\n",
    "\n",
    "Just how accurate is your machine-learning (ML) model? The question might seem trivial at first: just look at the percentage of predictions or classifications that it gets right. But examining only the accuracy score of an ML model tells only part of the story of its usefulness in the real world. All models produce false positives and false negatives; which of those should you tune your model to favor? If your model is to be used to identify individuals carrying an infectious disease, fewer false negatives at the cost of more false positives could be desirable. If you are making classifications in support of criminal cases, accepting more false negatives to avoid false positives might be essential.\n",
    "\n",
    "> **Learning objective:** By the end of this section, you should have a basic understanding of the different means of assessing the accuracy of an ML model and how the ROC curve can help you understand their relationship in your own models.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "To assess the accuracy of a model, we need to create and fit one first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare the data\n",
    "\n",
    "We'll again use the same dataset drawn from the [U.S. Department of Agriculture National Nutrient Database for Standard Reference](https://www.ars.usda.gov/northeast-area/beltsville-md-bhnrc/beltsville-human-nutrition-research-center/nutrient-data-laboratory/docs/usda-national-nutrient-database-for-standard-reference/) that you used in Sections 1.1 and 1.2. (Note that the path name is case sensitive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/USDA-nndb-combined.csv', encoding='latin_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model in this section doesn't like `NaN` values any more than principal-component analysis (PCA) did in Section 1.2, so we'll drop all rows with `NaN`s in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    ">\n",
    "> Drop all rows containing `NaN` values in `df`. (**Hint:** Refer to this page if you are unsure about which method to use to perform this action.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did in Section 1.2, let's divide our current dataset into descriptive and quantitative `DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_df = df.iloc[:, [0, 1, 2]+[i for i in range(50,54)]]\n",
    "desc_df.set_index('NDB_No', inplace=True)\n",
    "desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutr_df = df.iloc[:, :-5]\n",
    "nutr_df = nutr_df.drop(['FoodGroup', 'Shrt_Desc'], axis=1)\n",
    "nutr_df.set_index('NDB_No', inplace=True)\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at `nutr_df`. We'll use the `DataFrame` `describe()` method to do this, but, given the number of columns, we'll also want to transpose the `DataFrame` using the `T` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutr_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method provides a wealth of information about the `DataFrame`. For our purposes, it's particularly useful for identifying those columns whose means are much larger or much smaller than their medians (their respective 50th percentiles). Such differences indicate features in which a minority of comparatively large or small instances skew the means.\n",
    "\n",
    "In particular, several foods appear to be very low in calories, so much so that the mean of `Energ_Kcal` is close to its 25th percentile level, almost a full quartile away from where we might expect it, closer to its median value. As a result, `Energ_Kcal` will be the feature we'll later train our model to identify.\n",
    "\n",
    "Correlated features are still an issue, so we'll remove the `Folate_DFE_(µg)`, `Vit_A_RAE`, and `Vit_D_IU` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutr_df.drop(['Folate_DFE_(µg)', 'Vit_A_RAE', 'Vit_D_IU'], \n",
    "        inplace=True, axis=1)\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and fit the model\n",
    "\n",
    "Before we can fit a model to find low-calorie foods, we first must define numerically for the model what we mean by \"low-calorie.\" We do this by means of a dummy variable, in this case, a new column that reads `1` if the value in a row in `Energ_Kcal` is less than equal to the mean for that column and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nutr_df['Low_cal'] = np.where(nutr_df['Energ_Kcal']<=nutr_df['Energ_Kcal'].mean(), 1, 0)\n",
    "nutr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas creates our new column at the end of the `DataFrame`.\n",
    "\n",
    "Now to fit the model. Drawing from conventions established by the statistical community for regression models, we'll refer to our predictor features (otherwise known as the independent variables) as `X` and our response (the outcome or dependent variable) as `y`. Because we want to predict `Low_cal`, we cannot have it in `X`. Similarly, because `Low_cal` is really just a Boolean alias for `Energ_Kcal`, we should also remove `Energ_Kcal` from `X`. Finally, because the fat content of foods is generally so closely correlated with foods' energy content, we should also not include `Lipid_Tot_(g)` in `X`. (We want our model to be good but not too good to illustrate the ROC curve later in the section.)\n",
    "\n",
    "> **Technical note**\n",
    ">\n",
    "> Using capital `X` for the predictors and lower-case `y` for the response is a widely used convention in statistics and data science that we will use here and elsewhere in these courses.\n",
    "\n",
    "We'll set `y` to just be equal to `Low_cal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = nutr_df.drop(['Energ_Kcal', 'Low_cal', 'Lipid_Tot_(g)'], axis=1)\n",
    "y = nutr_df['Low_cal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now randomly split the data between training and test datasets. Scikit-learn's `train_test_split` is particularly convenient for this; use the `test_size` parameter to reserve 30 percent of the data for testing (and 70 percent for training). The `random_state` parameter ensures that training and test data split the same way each time so that these results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to fit the model against the training data and get predictions from it against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess the accuracy of the model\n",
    "\n",
    "Scikit-learn provides three means of assessing the accuracy of models: the accuracy score, the confusion matrix, and the classification report.\n",
    "\n",
    "Why so many ways of measuring accuracy for a model? Recall from the beginning of the section that success means different things in different contexts. In some cases, avoiding false negatives is paramount (such as when working with infectious diseases). In other cases, avoided false positives is essential (such as in instances involving the criminal-justice system). Other times, striking a balance of false positives and negatives is the best approach. These different measures provided the tools for data scientists to best tune their models for the problems at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy score\n",
    "\n",
    "Our accuracy score tells us the fraction of correctly classified samples: the sum of the number of correct true and false predictions divided by the number of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "The confusion matrix is another way to present this same information, this time with raw scores. The columns show the true condition, with positive on the left and negative on the right. The rows show predicted conditions, with positive on the top and negative on the bottom. So, the matrix below shows that our model correctly predicted 258 low-calorie foods (true positives) and incorrectly predicted another 23 (false positives). On the other hand, our model correctly predicted 371 higher-calorie foods (true negatives) and incorrectly predicted 5 more (false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test, predictions), \n",
    "             columns=['True low cal', 'True high cal'], \n",
    "             index=['Predicted low cal', 'Predicted high cal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification reports the proportions of low-calorie and high-calorie foods with four scores:\n",
    " - **Precision:** The number of true positives divided by the sum of true positives and false positives; closer to 1 is better.\n",
    " - **Recall:** The true-positive rate, which is the number of true positives divided by the sum of the true positives and the false negatives.\n",
    " - **F1 score:** The harmonic mean (the average for rates) of precision and recall.\n",
    " - **Support:** The number of true instances for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> **Takeaway:** This subsection gave you a taste of performing classification using logistic regression by removing extraneous variables, checking for multicollinearity, handling missing values, and fitting and evaluating your model. In the next subsection, we'll look at visualizing the accuracy of ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "\n",
    "The ROC curve was developed during World War II to measure the ability of radar-receiver operators to correctly identify aircraft, hence the name, \"receiver operating characteristic.\" From military application, the ROC curve made its way more broadly to signal-detection theory and from there to statistics and data science.\n",
    "\n",
    "ROC curves typically plot the true positive rate on the Y axis and the false positive rate on the X axis. This means that the top-left corner of the plot is the “ideal” point: a false positive rate of 0 (zero), and a true positive rate of 1. Such an ideal is rarely (if ever) realized in practice, but it is the theoretical goal.\n",
    "\n",
    "A key concept with ROC curves is *area under the curve* (AUC), with a larger AUC representing a better model. The slope of the ROC curve is also important because it's better to maximize the true positive rate while minimizing the false positive rate.\n",
    "\n",
    "Let's find the ROC curve and AUC for the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to graph the ROC curve, we need to find the true positive rate (TPR) and false positive rate (FPR) at all thresholds of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(X_test)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict_proba()` function returns estimates for both classes (high-calorie and low-calorie) for every instance in `X_test`. Because each outputted pair of numbers adds up to 1, we really only need to retain one half of each pair. We'll go with the low-calorie probabilities and we can redefine `probs` to just contain those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = probs[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `roc_curve()` function then compares the actual values (`0` or `1`) from `y_test` against the probabilities generated by the model (`probs`) and returns the FPR, the TPR, and the thresholds (the instances of prediction by the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `auc()` function computes the area under the ROC curve for our FPR and TPR values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rocauc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has an AUC of 0.99, which is higher than our accuracy rate of 0.96. Not only is our model accurate, but it achieves high rates of true positive classifications with very low rates of false positive classifications.\n",
    "\n",
    "Let's see this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic (ROC) metric')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % rocauc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Takeaway:** Because no ML model is perfect, it's important to know a model's TPR and FPR. The ROC curve (and the AUC figure) provide a concise means of summarizing a lot of nuance about your model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
